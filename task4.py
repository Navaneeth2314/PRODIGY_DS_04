# -*- coding: utf-8 -*-
"""Task4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qHlEjINULHG1bTDV-p1FOb13fwYjBkoE
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from textblob import TextBlob
from nltk.tokenize import word_tokenize,sent_tokenize
from nltk.corpus import stopwords
from nltk import pos_tag

import nltk
nltk.download('stopwords')

tdata = pd.read_csv('twitter_training.csv')
data  = pd.read_csv('twitter_validation.csv')

tdata

data

tdata.columns = ['id','game','sentiment','text']
data.columns = ['id','game','sentiment','text']

tdata

data

tdata.shape

tdata.columns

tdata.describe(include='all')

id_types = tdata['id'].value_counts()
id_types

plt.figure(figsize=(10,6))
sns.barplot(y=id_types.index,x = id_types.values)
plt.xlabel('Type')
plt.ylabel('Count')
plt.title('# of id vs count ')
plt.show

game_types = tdata['game'].value_counts()
game_types

plt.figure(figsize=(14,10))
sns.barplot(y=game_types.index,x = game_types.values)
plt.xlabel('Type')
plt.ylabel('Count')
plt.title('# of game vs count ')
plt.show

total_null = tdata.isnull().sum().sort_values(ascending=False)
percent_null = ((tdata.isnull().sum()/tdata.isnull().count())*100).sort_values(ascending=False)
print("total records  = ",data.shape[0])
missing_data = pd.concat([total_null,percent_null.round(2)],axis=1,keys=['Total missing ',' in Percent'])
missing_data

tdata.dropna(subset=['text'],inplace=True)

total_null=tdata.isnull().sum().sort_values(ascending=False)
percent_null = ((tdata.isnull().sum()/tdata.isnull().count())*100).sort_values(ascending=False)
print("total records  = ",tdata.shape[0])
missing_data = pd.concat([total_null,percent_null.round(2)],axis=1,keys=['Total missing ',' in Percent'])
missing_data

train0 = tdata[tdata['sentiment'] == "Negtive"]
train1 = tdata[tdata['sentiment'] == "postive"]
train2 = tdata[tdata['sentiment'] == "irrelevent"]
train3 = tdata[tdata['sentiment'] == "netural"]

train0.shape,train1.shape,train2.shape,train3.shape

train0 = train0[:int (train0.shape[0]/12)]
train1 = train1[:int (train0.shape[0]/12)]
train2 = train2[:int (train0.shape[0]/12)]
train3 = train3[:int (train0.shape[0]/12)]

train0.shape,train1.shape,train2.shape,train3.shape

tdata = pd.concat([train0,train1,train2,train3],axis=0)
tdata

id_types = tdata['id'].value_counts()
id_types

